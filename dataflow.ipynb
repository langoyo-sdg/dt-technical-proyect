{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from input.Reader import Reader\n",
    "import logging\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_file = \"./metadata/conf.json\"\n",
    "logs_file = \"./Logs/application_logs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logging system\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level\n",
    "    format= \"%(asctime)s - \" + f\"{metadata_file}\" + \" - %(name)s - %(levelname)s - %(message)s\",  # Log message format\n",
    "    handlers=[\n",
    "        logging.FileHandler(logs_file),  # Dump logs to a file\n",
    "        logging.StreamHandler()  # Also print logs to the console\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Dataflow\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(metadata_file, 'r') as file:\n",
    "    metadata = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 13:42:33,547 - ./metadata/conf.json - Reader - INFO - Schema conversion successful: StructType([StructField('name', StringType(), nullable=True),StructField('age', IntegerType(), nullable=True),StructField('office', StringType(), nullable=True)])\n",
      "2024-12-18 13:42:33,549 - ./metadata/conf.json - Reader - INFO - Options rertrieved: {'dropFieldIfAllNull': False}\n",
      "2024-12-18 13:42:35,767 - ./metadata/conf.json - Reader - INFO - Data sources loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "reader = Reader(spark)\n",
    "sources = reader.load_df(metadata['sources'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+\n",
      "|    name| age|   office|\n",
      "+--------+----+---------+\n",
      "|  xavier|  32|barcelona|\n",
      "|  miguel|  12|santander|\n",
      "|  manuel|  56|   murcia|\n",
      "|  miguel|  56|         |\n",
      "|ricardio|NULL|   murcia|\n",
      "|    juan|  45|   getafe|\n",
      "| ricardo|  37| valencia|\n",
      "|    fran|  29| alicante|\n",
      "+--------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sources['person_inputs'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 13:42:40,680 - ./metadata/conf.json - TransformationManager - INFO - Applying transformation: validate_fields with params: {'validations': [{'field': 'office', 'validations': ['notEmpty']}, {'field': 'age', 'validations': ['notNull']}]}\n",
      "2024-12-18 13:42:40,917 - ./metadata/conf.json - TransformationManager - INFO - Applying transformation: add_fields with params: {'addFields': [{'name': 'dt', 'function': 'current_timestamp'}]}\n",
      "2024-12-18 13:42:40,918 - ./metadata/conf.json - AddFields - INFO - Adding current timestamp column 'dt'.\n",
      "2024-12-18 13:42:40,932 - ./metadata/conf.json - AddFields - INFO - Added field 'dt' with function 'current_timestamp'.\n",
      "2024-12-18 13:42:40,935 - ./metadata/conf.json - TransformationManager - INFO - All transformations were applied successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformations.TransformationManager import TransformationManager\n",
    "transformator = TransformationManager()\n",
    "for source_name, source in sources.items():\n",
    "    sources[source_name] = transformator.apply_transformations(df=source,transformations=metadata['transformations'], input_name=source_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+--------------------+--------------------+\n",
      "|    name| age|   office|   validation_errors|                  dt|\n",
      "+--------+----+---------+--------------------+--------------------+\n",
      "|  xavier|  32|barcelona|                  []|2024-12-18 13:42:...|\n",
      "|  miguel|  12|santander|                  []|2024-12-18 13:42:...|\n",
      "|  manuel|  56|   murcia|                  []|2024-12-18 13:42:...|\n",
      "|  miguel|  56|         |[notEmpty: office...|2024-12-18 13:42:...|\n",
      "|ricardio|NULL|   murcia|[notNull: age mus...|2024-12-18 13:42:...|\n",
      "|    juan|  45|   getafe|                  []|2024-12-18 13:42:...|\n",
      "| ricardo|  37| valencia|                  []|2024-12-18 13:42:...|\n",
      "|    fran|  29| alicante|                  []|2024-12-18 13:42:...|\n",
      "+--------+----+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sources['person_inputs'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 13:42:41,690 - ./metadata/conf.json - Writer - INFO - Processing sink 'ok' for input 'person_inputs'.\n",
      "2024-12-18 13:42:41,692 - ./metadata/conf.json - Writer - INFO - Writing DataFrame to path: ./data/output/ok/person with format: csv and save mode: OVERWRITE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 13:42:42,930 - ./metadata/conf.json - Writer - INFO - Successfully wrote DataFrame to path: ./data/output/ok/person\n",
      "2024-12-18 13:42:42,932 - ./metadata/conf.json - Writer - INFO - Successfully processed sink 'ok' for input 'person_inputs'.\n",
      "2024-12-18 13:42:42,978 - ./metadata/conf.json - Writer - INFO - Processing sink 'ko' for input 'person_inputs'.\n",
      "2024-12-18 13:42:42,979 - ./metadata/conf.json - Writer - INFO - Writing DataFrame to path: ./data/output/ko/person with format: json and save mode: OVERWRITE\n",
      "2024-12-18 13:42:43,471 - ./metadata/conf.json - Writer - INFO - Successfully wrote DataFrame to path: ./data/output/ko/person\n",
      "2024-12-18 13:42:43,474 - ./metadata/conf.json - Writer - INFO - Successfully processed sink 'ko' for input 'person_inputs'.\n"
     ]
    }
   ],
   "source": [
    "from output.Writer import Writer\n",
    "writer = Writer(spark_session=spark)\n",
    "writer.write_dataframes(sources, metadata['sinks'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
