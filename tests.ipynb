{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"NotebookTests\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+\n",
      "|    name| age|   office|\n",
      "+--------+----+---------+\n",
      "|  xavier|  32|barcelona|\n",
      "|  miguel|  12|santander|\n",
      "|  manuel|  56|   murcia|\n",
      "|  miguel|  56|         |\n",
      "|ricardio|NULL|   murcia|\n",
      "|  xavier|  32|     NULL|\n",
      "|  miguel|  12|santander|\n",
      "|    NULL|  56|   murcia|\n",
      "|  miguel|NULL|         |\n",
      "|ricardio|NULL|   murcia|\n",
      "|    juan|  45|   getafe|\n",
      "| ricardo|  37| valencia|\n",
      "|    fran|  29| alicante|\n",
      "+--------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from input.Reader import Reader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define metadata for testing\n",
    "sources_metadata =[\n",
    "      {\n",
    "        \"name\": \"person_inputs\",\n",
    "        \"paths\": [\n",
    "          \"C:/Users/andres.langoyo/Documents/prueba_tecnica/dataflow/dt-technical-proyect/data/input/person\"\n",
    "      ],\n",
    "        \"format\": \"JSON\",\n",
    "        \"schema\":[\n",
    "          {\"field\":\"name\", \"type\":\"STRING\"},\n",
    "          {\"field\":\"age\", \"type\":\"INTEGER\"},\n",
    "          {\"field\":\"office\", \"type\":\"STRING\"}\n",
    "          ],\n",
    "          \n",
    "      }\n",
    "    ]\n",
    "\n",
    "# Initialize Reader and load DataFrame\n",
    "reader = Reader(spark)\n",
    "sources = reader.load_df(sources_metadata)\n",
    "\n",
    "# Validate output\n",
    "assert \"person_inputs\" in sources, \"Source name not found in output\"\n",
    "df = sources[\"person_inputs\"]\n",
    "assert df is not None, \"DataFrame should not be None\"\n",
    "assert df.columns == [\"name\", \"age\", \"office\"], \"Unexpected columns in DataFrame\"\n",
    "assert [field.dataType for field in df.schema] == [StringType(), IntegerType(), StringType()], \"Wrong schhema types\"\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong input path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input.Reader import Reader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define metadata for testing\n",
    "sources_metadata =[\n",
    "      {\n",
    "        \"name\": \"person_inputs\",\n",
    "        \"paths\": [\n",
    "          \"./data/input/person/p_1.json\",\n",
    "      ],\n",
    "        \"format\": \"CSV\",\n",
    "        \"schema\":[\n",
    "          {\"field\":\"name\", \"type\":\"STRING\"},\n",
    "          {\"field\":\"age\", \"type\":\"INTEGER\"},\n",
    "          {\"field\":\"office\", \"type\":\"STRING\"}\n",
    "          ],\n",
    "          \n",
    "      }\n",
    "    ]\n",
    "\n",
    "try:\n",
    "  reader = Reader(spark)\n",
    "  sources = reader.load_df(sources_metadata)\n",
    "  sources['person_inputs'].show()\n",
    "except Exception as e:\n",
    "  assert \"Error reading sources\" in str(e), \"not existing path not caught\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malformed input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in schema conversion: 'type'\n"
     ]
    }
   ],
   "source": [
    "from input.Reader import Reader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define metadata for testing\n",
    "sources_metadata =[\n",
    "      {\n",
    "        \"name\": \"person_inputs\",\n",
    "        \"paths\": [\n",
    "          \"./data/input/person/people_1.json\",\n",
    "      ],\n",
    "        \"format\": \"CSV\",\n",
    "        \"schema\":[\n",
    "          {\"field\":\"name\"},\n",
    "          {\"field\":\"age\", \"type\":\"INTEGER\"},\n",
    "          {\"field\":\"office\", \"type\":\"STRING\"}\n",
    "          ],\n",
    "          \n",
    "      }\n",
    "    ]\n",
    "\n",
    "try:\n",
    "  reader = Reader(spark)\n",
    "  sources = reader.load_df(sources_metadata)\n",
    "  sources['person_inputs'].show()\n",
    "except Exception as e:\n",
    "    assert \"Error reading sources\" in str(e), \"badly formed schema not caught\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corrupted json input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+\n",
      "|  name| age|   office|\n",
      "+------+----+---------+\n",
      "|xavier|  32|     NULL|\n",
      "|miguel|  12|santander|\n",
      "|  NULL|  56|   murcia|\n",
      "|  NULL|NULL|     NULL|\n",
      "|  NULL|NULL|     NULL|\n",
      "+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from input.Reader import Reader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define metadata for testing\n",
    "sources_metadata =[\n",
    "      {\n",
    "        \"name\": \"person_inputs\",\n",
    "        \"paths\": [\n",
    "          \"./data/input/person/corrupted_input.json\",\n",
    "      ],\n",
    "        \"format\": \"JSON\",\n",
    "        \"schema\":[\n",
    "          {\"field\":\"name\", \"type\": \"STRING\"},\n",
    "          {\"field\":\"age\", \"type\":\"INTEGER\"},\n",
    "          {\"field\":\"office\", \"type\":\"STRING\"}\n",
    "          ],\n",
    "          \n",
    "      }\n",
    "    ]\n",
    "\n",
    "reader = Reader(spark)\n",
    "sources = reader.load_df(sources_metadata)\n",
    "sources['person_inputs'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "options in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   name|age|   office|\n",
      "+-------+---+---------+\n",
      "| xavier| 32|barcelona|\n",
      "| miguel| 12|santander|\n",
      "| manuel| 56|   murcia|\n",
      "|   juan| 45|   getafe|\n",
      "|ricardo| 37| valencia|\n",
      "|   fran| 29| alicante|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from input.Reader import Reader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define metadata for testing\n",
    "sources_metadata =[\n",
    "      {\n",
    "        \"name\": \"person_inputs\",\n",
    "        \"paths\": [\n",
    "          \"./data/input/person/input_csv.csv\"\n",
    "      ],\n",
    "        \"format\": \"CSV\",\n",
    "        \"schema\":[],\n",
    "          \"options\":{\n",
    "              \"inferSchema\": \"true\",\n",
    "              \"header\":\"true\"\n",
    "          }\n",
    "          \n",
    "      }\n",
    "    ]\n",
    "\n",
    "reader = Reader(spark)\n",
    "sources = reader.load_df(sources_metadata)\n",
    "df = sources['person_inputs']\n",
    "assert df.columns == [\"name\", \"age\", \"office\"], \"Unexpected columns in DataFrame\"\n",
    "assert [field.dataType for field in df.schema] == [StringType(), IntegerType(), StringType()], \"Wrong schhema types\"\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----------+--------------------+--------------------+\n",
      "|name| age|     office|   validation_errors|                  dt|\n",
      "+----+----+-----------+--------------------+--------------------+\n",
      "|John|  30|   new york|                  []|2024-12-20 11:28:...|\n",
      "|Jane|NULL|new orleans|[notNull: age mus...|2024-12-20 11:28:...|\n",
      "|    |  25|     berlin|[notEmpty: name m...|2024-12-20 11:28:...|\n",
      "|    |NULL|     munich|[notEmpty: name m...|2024-12-20 11:28:...|\n",
      "+----+----+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformations.TransformationManager import TransformationManager\n",
    "from pyspark.sql.functions import col\n",
    "# Sample data and schema\n",
    "data = [(\"John\", 30, \"new york\"), (\"Jane\", None, \"new orleans\"), (\"\", 25, \"berlin\"), (\"\", None, \"munich\")]\n",
    "schema = [\"name\", \"age\", \"office\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Initialize TransformationManager and apply transformations\n",
    "transformator = TransformationManager()\n",
    "transformations = [\n",
    "    {\n",
    "        \"name\": \"validate_fields\",\n",
    "        \"type\": \"validate_fields\",\n",
    "        \"input\": \"test_input\",\n",
    "        \"params\": {\n",
    "            \"validations\": [\n",
    "                {\"field\": \"name\", \"validations\": [\"notEmpty\"]},\n",
    "                {\"field\": \"age\", \"validations\": [\"notNull\"]}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ok_with_date\",\n",
    "        \"type\": \"add_fields\",\n",
    "        \"input\": \"test_input\",\n",
    "        \"params\": {\n",
    "          \"addFields\": [\n",
    "            {\n",
    "              \"name\": \"dt\",\n",
    "              \"function\": \"current_timestamp\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "]\n",
    "\n",
    "transformed_df = transformator.apply_transformations(df, transformations, \"test_input\")\n",
    "# Validate transformation\n",
    "assert \"validation_errors\" in transformed_df.columns, \"Validation errors column missing\"\n",
    "assert transformed_df.filter((col(\"office\") == \"new york\") & (col(\"validation_errors\") == \"[]\")).count() == 1, \"validation error should be empty in correct row\"\n",
    "assert transformed_df.filter((col(\"office\") == \"new orleans\") & (col(\"validation_errors\").contains(\"notNull\"))).count() == 1, \"validation error should contain a not null error\"\n",
    "assert transformed_df.filter((col(\"office\") == \"berlin\") & (col(\"validation_errors\").contains(\"notEmpty\"))).count() == 1, \"validation error should contain a not empty error\"\n",
    "assert transformed_df.filter((col(\"office\") == \"munich\") & (col(\"validation_errors\").contains(\"notEmpty\")) & (col(\"validation_errors\").contains(\"notNull\"))).count() == 1, \"validation error should contain a not null and not empty error\"\n",
    "assert \"dt\" in transformed_df.columns, \"Timestamp column missing\"\n",
    "\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non existing validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown validation rule: weirdVal\n",
      "Error during validation: Unknown validation rule: weirdVal\n",
      "Error applying transformation validate_fields: Error during validation: Unknown validation rule: weirdVal\n"
     ]
    }
   ],
   "source": [
    "from transformations.TransformationManager import TransformationManager\n",
    "from pyspark.sql.functions import col\n",
    "# Sample data and schema\n",
    "data = [(\"John\", 30, \"new york\"), (\"Jane\", None, \"new orleans\"), (\"\", 25, \"berlin\"), (\"\", None, \"munich\")]\n",
    "schema = [\"name\", \"age\", \"office\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Initialize TransformationManager and apply transformations\n",
    "transformator = TransformationManager()\n",
    "transformations = [\n",
    "    {\n",
    "        \"name\": \"validate_fields\",\n",
    "        \"type\": \"validate_fields\",\n",
    "        \"input\": \"test_input\",\n",
    "        \"params\": {\n",
    "            \"validations\": [\n",
    "                {\"field\": \"name\", \"validations\": [\"notEmpty\"]},\n",
    "                {\"field\": \"age\", \"validations\": [\"weirdVal\"]}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ok_with_date\",\n",
    "        \"type\": \"add_fields\",\n",
    "        \"input\": \"test_input\",\n",
    "        \"params\": {\n",
    "          \"addFields\": [\n",
    "            {\n",
    "              \"name\": \"dt\",\n",
    "              \"function\": \"current_timestamp\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "]\n",
    "try:\n",
    "    transformed_df = transformator.apply_transformations(df, transformations, \"test_input\")\n",
    "except Exception as e:\n",
    "    assert \"Unknown validation rule\" in str(e), \"unknown validation error not caught\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unkown transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown transformation rule: delete_column\n",
      "Error applying transformation delete_column: Unknown transformation rule: delete_column\n"
     ]
    }
   ],
   "source": [
    "from transformations.TransformationManager import TransformationManager\n",
    "from pyspark.sql.functions import col\n",
    "# Sample data and schema\n",
    "data = [(\"John\", 30, \"new york\"), (\"Jane\", None, \"new orleans\"), (\"\", 25, \"berlin\"), (\"\", None, \"munich\")]\n",
    "schema = [\"name\", \"age\", \"office\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Initialize TransformationManager and apply transformations\n",
    "transformator = TransformationManager()\n",
    "transformations = [\n",
    "    {\n",
    "        \"name\": \"validate_fields\",\n",
    "        \"type\": \"validate_fields\",\n",
    "        \"input\": \"test_input\",\n",
    "        \"params\": {\n",
    "            \"validations\": [\n",
    "                {\"field\": \"name\", \"validations\": [\"notEmpty\"]},\n",
    "                {\"field\": \"age\", \"validations\": [\"notNull\"]}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ok_with_date\",\n",
    "        \"type\": \"delete_column\",\n",
    "        \"input\": \"test_input\",\n",
    "        \"params\": {\n",
    "          \"delete_column\": ['age']\n",
    "        }\n",
    "      }\n",
    "]\n",
    "try:\n",
    "    transformed_df = transformator.apply_transformations(df, transformations, \"test_input\")\n",
    "except Exception as e:\n",
    "    assert \"Unknown transformation rule\" in str(e), \"unknown validation error not caught\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writer testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it writes to the three possible paths ok, ko, default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[name: string, age: int, office: string, dt: timestamp] csv ['./data/output/ok/person']\n",
      "DataFrame[name: string, age: int, office: string, validation_errors: string, dt: timestamp] json ['./data/output/ko/person']\n",
      "DataFrame[name: string, age: int, office: string] parquet ['./data/output/all/person']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "from input.Reader import Reader\n",
    "from transformations.TransformationManager import TransformationManager\n",
    "from output.Writer import Writer\n",
    "\n",
    "shutil.rmtree(\"./data/output\")\n",
    "\n",
    "# Load metadata for the workflow\n",
    "metadata_path = \"./metadata/test_writer.json\"\n",
    "with open(metadata_path, \"r\") as file:\n",
    "    metadata = json.load(file)\n",
    "\n",
    "# Initialize components\n",
    "reader = Reader(spark)\n",
    "transformator = TransformationManager()\n",
    "writer = Writer(spark)\n",
    "\n",
    "# Execute the workflow\n",
    "sources = reader.load_df(metadata[\"sources\"])\n",
    "\n",
    "# Apply transformations\n",
    "for source_name, source in sources.items():\n",
    "    sources[source_name] = transformator.apply_transformations(\n",
    "        df=source, \n",
    "        transformations=metadata[\"transformations\"], \n",
    "        input_name=source_name\n",
    "    )\n",
    "\n",
    "# Write data to sinks\n",
    "writer.write_dataframes(sources, metadata[\"sinks\"])\n",
    "\n",
    "# Validate outputs\n",
    "output_ok = \"./data/output/ok/person\"\n",
    "output_ko = \"./data/output/ko/person\"\n",
    "all = \"./data/output/all/person\"\n",
    "assert os.path.exists(output_ok), \"Output for valid data (ok) was not created\"\n",
    "assert os.path.exists(output_ko), \"Output for invalid data (ko) was not created\"\n",
    "assert os.path.exists(all), \"Output for default data (wrong path) was not created\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End to end test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andres.langoyo\\Documents\\prueba_tecnica\\dataflow\\dt-technical-proyect\n",
      "CompletedProcess(args=['c:\\\\Users\\\\andres.langoyo\\\\Documents\\\\prueba_tecnica\\\\dataflow\\\\dt-technical-proyect\\\\.venv\\\\Scripts\\\\python.exe', './dataflow.py', '-m', './metadata/conf.json', '-l', './logs/tests_logs.txt'], returncode=0, stdout=b\"DataFrame[name: string, age: int, office: string, dt: timestamp] csv ['./data/output/ok/person']\\r\\nDataFrame[name: string, age: int, office: string, validation_errors: string, dt: timestamp] json ['./data/output/ko/person']\\r\\nCORRECTO: el proceso con PID 30320 (proceso secundario de PID 20548)\\r\\nha sido terminado.\\r\\nCORRECTO: el proceso con PID 20548 (proceso secundario de PID 11516)\\r\\nha sido terminado.\\r\\nCORRECTO: el proceso con PID 11516 (proceso secundario de PID 14844)\\r\\nha sido terminado.\\r\\n\", stderr=b'Setting default log level to \"WARN\".\\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\\r\\n24/12/20 11:50:03 WARN Utils: Service \\'SparkUI\\' could not bind on port 4040. Attempting port 4041.\\r\\n24/12/20 11:50:03 WARN Utils: Service \\'SparkUI\\' could not bind on port 4041. Attempting port 4042.\\r\\n2024-12-20 11:50:04,200 - ./metadata/conf.json - Reader - INFO - Schema conversion successful: StructType([StructField(\\'name\\', StringType(), nullable=True),StructField(\\'age\\', IntegerType(), nullable=True),StructField(\\'office\\', StringType(), nullable=True)])\\r\\n2024-12-20 11:50:04,201 - ./metadata/conf.json - Reader - INFO - Options rertrieved: {\\'dropFieldIfAllNull\\': False}\\r\\n2024-12-20 11:50:06,786 - ./metadata/conf.json - Reader - INFO - Data sources loaded successfully.\\r\\n2024-12-20 11:50:06,786 - ./metadata/conf.json - TransformationManager - INFO - Applying transformation: validate_fields with params: {\\'validations\\': [{\\'field\\': \\'office\\', \\'validations\\': [\\'notEmpty\\']}, {\\'field\\': \\'age\\', \\'validations\\': [\\'notNull\\']}]}\\r\\n2024-12-20 11:50:07,106 - ./metadata/conf.json - TransformationManager - INFO - Applying transformation: add_fields with params: {\\'addFields\\': [{\\'name\\': \\'dt\\', \\'function\\': \\'current_timestamp\\'}]}\\r\\n2024-12-20 11:50:07,107 - ./metadata/conf.json - AddFields - INFO - Adding current timestamp column \\'dt\\'.\\r\\n2024-12-20 11:50:07,148 - ./metadata/conf.json - AddFields - INFO - Added field \\'dt\\' with function \\'current_timestamp\\'.\\r\\n2024-12-20 11:50:07,149 - ./metadata/conf.json - TransformationManager - INFO - All transformations were applied successfully.\\r\\n2024-12-20 11:50:07,224 - ./metadata/conf.json - Writer - INFO - Processing sink \\'ok\\' for input \\'person_inputs\\'.\\r\\n2024-12-20 11:50:07,228 - ./metadata/conf.json - Writer - INFO - Writing DataFrame to path: ./data/output/ok/person with format: csv and save mode: OVERWRITE\\r\\n\\r[Stage 0:>                                                          (0 + 1) / 1]\\r\\r                                                                                \\r2024-12-20 11:50:10,318 - ./metadata/conf.json - Writer - INFO - Successfully wrote DataFrame to path: ./data/output/ok/person\\r\\n2024-12-20 11:50:10,318 - ./metadata/conf.json - Writer - INFO - Successfully processed sink \\'ok\\' for input \\'person_inputs\\'.\\r\\n2024-12-20 11:50:10,359 - ./metadata/conf.json - Writer - INFO - Processing sink \\'ko\\' for input \\'person_inputs\\'.\\r\\n2024-12-20 11:50:10,359 - ./metadata/conf.json - Writer - INFO - Writing DataFrame to path: ./data/output/ko/person with format: json and save mode: OVERWRITE\\r\\n2024-12-20 11:50:10,899 - ./metadata/conf.json - Writer - INFO - Successfully wrote DataFrame to path: ./data/output/ko/person\\r\\n2024-12-20 11:50:10,899 - ./metadata/conf.json - Writer - INFO - Successfully processed sink \\'ko\\' for input \\'person_inputs\\'.\\r\\n2024-12-20 11:50:10,918 - ./metadata/conf.json - py4j.clientserver - INFO - Closing down clientserver connection\\r\\n')\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "metadata_path = \"./metadata/conf.json\"\n",
    "logs_path = \"./logs/tests_logs.txt\"\n",
    "print(os.path.abspath('.'))\n",
    "\n",
    "# Run the main script\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"./dataflow.py\", \"-m\", metadata_path, \"-l\", logs_path],\n",
    "    capture_output=True,\n",
    "    cwd=os.path.dirname(os.path.abspath('./dt-technical-proyect'))\n",
    ")\n",
    "print(result)\n",
    "# Check the logs for success\n",
    "assert result.returncode == 0, \"error occurred while running the subprocess\"\n",
    "with open(logs_path, \"r\") as log_file:\n",
    "    logs = log_file.read()\n",
    "    assert \"Successfully processed sink\" in logs, \"process didn't finish successfully\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
