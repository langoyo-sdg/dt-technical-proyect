{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"NotebookTests\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+\n",
      "|    name| age|   office|\n",
      "+--------+----+---------+\n",
      "|  xavier|  32|barcelona|\n",
      "|  miguel|  12|santander|\n",
      "|  manuel|  56|   murcia|\n",
      "|  miguel|  56|         |\n",
      "|ricardio|NULL|   murcia|\n",
      "|    juan|  45|   getafe|\n",
      "| ricardo|  37| valencia|\n",
      "|    fran|  29| alicante|\n",
      "+--------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from input.Reader import Reader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define metadata for testing\n",
    "sources_metadata =[\n",
    "      {\n",
    "        \"name\": \"person_inputs\",\n",
    "        \"paths\": [\n",
    "          \"C:/Users/andres.langoyo/Documents/prueba_tecnica/dataflow/dt-technical-proyect/data/input/person\"\n",
    "      ],\n",
    "        \"format\": \"JSON\",\n",
    "        \"schema\":[\n",
    "          {\"field\":\"name\", \"type\":\"STRING\"},\n",
    "          {\"field\":\"age\", \"type\":\"INTEGER\"},\n",
    "          {\"field\":\"office\", \"type\":\"STRING\"}\n",
    "          ],\n",
    "          \n",
    "      }\n",
    "    ]\n",
    "\n",
    "# Initialize Reader and load DataFrame\n",
    "reader = Reader(spark)\n",
    "sources = reader.load_df(sources_metadata)\n",
    "\n",
    "# Validate output\n",
    "assert \"person_inputs\" in sources, \"Source name not found in output\"\n",
    "df = sources[\"person_inputs\"]\n",
    "assert df is not None, \"DataFrame should not be None\"\n",
    "assert df.columns == [\"name\", \"age\", \"office\"], \"Unexpected columns in DataFrame\"\n",
    "assert [field.dataType for field in df.schema] == [StringType(), IntegerType(), StringType()], \"Wrong schhema types\"\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong input path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from input.Reader import Reader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define metadata for testing\n",
    "sources_metadata =[\n",
    "      {\n",
    "        \"name\": \"person_inputs\",\n",
    "        \"paths\": [\n",
    "          \"./data/input/person/p_1.json\",\n",
    "      ],\n",
    "        \"format\": \"CSV\",\n",
    "        \"schema\":[\n",
    "          {\"field\":\"name\", \"type\":\"STRING\"},\n",
    "          {\"field\":\"age\", \"type\":\"INTEGER\"},\n",
    "          {\"field\":\"office\", \"type\":\"STRING\"}\n",
    "          ],\n",
    "          \n",
    "      }\n",
    "    ]\n",
    "\n",
    "try:\n",
    "  reader = Reader(spark)\n",
    "  sources = reader.load_df(sources_metadata)\n",
    "  sources['person_inputs'].show()\n",
    "except Exception as e:\n",
    "  assert \"Error reading sources\" in str(e), \"not existing path not caught\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malformed input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in schema conversion: 'type'\n"
     ]
    }
   ],
   "source": [
    "from input.Reader import Reader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define metadata for testing\n",
    "sources_metadata =[\n",
    "      {\n",
    "        \"name\": \"person_inputs\",\n",
    "        \"paths\": [\n",
    "          \"./data/input/person/people_1.json\",\n",
    "      ],\n",
    "        \"format\": \"CSV\",\n",
    "        \"schema\":[\n",
    "          {\"field\":\"name\"},\n",
    "          {\"field\":\"age\", \"type\":\"INTEGER\"},\n",
    "          {\"field\":\"office\", \"type\":\"STRING\"}\n",
    "          ],\n",
    "          \n",
    "      }\n",
    "    ]\n",
    "\n",
    "try:\n",
    "  reader = Reader(spark)\n",
    "  sources = reader.load_df(sources_metadata)\n",
    "  sources['person_inputs'].show()\n",
    "except Exception as e:\n",
    "    assert \"Error reading sources\" in str(e), \"badly formed schema not caught\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corrupted json input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error reading sources: [PATH_NOT_FOUND] Path does not exist: file:/c:/Users/andres.langoyo/Documents/prueba_tecnica/dataflow/dt-technical-proyect/data/input/person/corrupted_input.json.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\andres.langoyo\\Documents\\prueba_tecnica\\dataflow\\dt-technical-proyect\\input\\Reader.py:109\u001b[0m, in \u001b[0;36mReader.load_df\u001b[1;34m(self, metadata_sources)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema:\n\u001b[1;32m--> 109\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpaths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\andres.langoyo\\Documents\\prueba_tecnica\\dataflow\\dt-technical-proyect\\.venv\\lib\\site-packages\\pyspark\\sql\\readwriter.py:312\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\andres.langoyo\\Documents\\prueba_tecnica\\dataflow\\dt-technical-proyect\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\Users\\andres.langoyo\\Documents\\prueba_tecnica\\dataflow\\dt-technical-proyect\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/c:/Users/andres.langoyo/Documents/prueba_tecnica/dataflow/dt-technical-proyect/data/input/person/corrupted_input.json.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 22\u001b[0m\n\u001b[0;32m      5\u001b[0m sources_metadata \u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      6\u001b[0m       {\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperson_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m       }\n\u001b[0;32m     19\u001b[0m     ]\n\u001b[0;32m     21\u001b[0m reader \u001b[38;5;241m=\u001b[39m Reader(spark)\n\u001b[1;32m---> 22\u001b[0m sources \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43msources_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m sources[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\andres.langoyo\\Documents\\prueba_tecnica\\dataflow\\dt-technical-proyect\\input\\Reader.py:120\u001b[0m, in \u001b[0;36mReader.load_df\u001b[1;34m(self, metadata_sources)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading sources: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading sources: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error reading sources: [PATH_NOT_FOUND] Path does not exist: file:/c:/Users/andres.langoyo/Documents/prueba_tecnica/dataflow/dt-technical-proyect/data/input/person/corrupted_input.json."
     ]
    }
   ],
   "source": [
    "from input.Reader import Reader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define metadata for testing\n",
    "sources_metadata =[\n",
    "      {\n",
    "        \"name\": \"person_inputs\",\n",
    "        \"paths\": [\n",
    "          \"./data/input/corrupted_input.json\",\n",
    "      ],\n",
    "        \"format\": \"JSON\",\n",
    "        \"schema\":[\n",
    "          {\"field\":\"name\", \"type\": \"STRING\"},\n",
    "          {\"field\":\"age\", \"type\":\"INTEGER\"},\n",
    "          {\"field\":\"office\", \"type\":\"STRING\"}\n",
    "          ],\n",
    "          \n",
    "      }\n",
    "    ]\n",
    "\n",
    "reader = Reader(spark)\n",
    "sources = reader.load_df(sources_metadata)\n",
    "sources['person_inputs'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "options in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+\n",
      "|   name|age|   office|\n",
      "+-------+---+---------+\n",
      "| xavier| 32|barcelona|\n",
      "| miguel| 12|santander|\n",
      "| manuel| 56|   murcia|\n",
      "|   juan| 45|   getafe|\n",
      "|ricardo| 37| valencia|\n",
      "|   fran| 29| alicante|\n",
      "+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from input.Reader import Reader\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define metadata for testing\n",
    "sources_metadata =[\n",
    "      {\n",
    "        \"name\": \"person_inputs\",\n",
    "        \"paths\": [\n",
    "          \"./data/input/input_csv.csv\"\n",
    "      ],\n",
    "        \"format\": \"CSV\",\n",
    "        \"schema\":[],\n",
    "          \"options\":{\n",
    "              \"inferSchema\": \"true\",\n",
    "              \"header\":\"true\"\n",
    "          }\n",
    "          \n",
    "      }\n",
    "    ]\n",
    "\n",
    "reader = Reader(spark)\n",
    "sources = reader.load_df(sources_metadata)\n",
    "df = sources['person_inputs']\n",
    "assert df.columns == [\"name\", \"age\", \"office\"], \"Unexpected columns in DataFrame\"\n",
    "assert [field.dataType for field in df.schema] == [StringType(), IntegerType(), StringType()], \"Wrong schhema types\"\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----------+--------------------+--------------------+\n",
      "|name| age|     office|   validation_errors|                  dt|\n",
      "+----+----+-----------+--------------------+--------------------+\n",
      "|John|  30|   new york|                  []|2024-12-20 13:54:...|\n",
      "|Jane|NULL|new orleans|[notNull: age mus...|2024-12-20 13:54:...|\n",
      "|    |  25|     berlin|[notEmpty: name m...|2024-12-20 13:54:...|\n",
      "|    |NULL|     munich|[notEmpty: name m...|2024-12-20 13:54:...|\n",
      "+----+----+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformations.TransformationManager import TransformationManager\n",
    "from pyspark.sql.functions import col\n",
    "# Sample data and schema\n",
    "data = [(\"John\", 30, \"new york\"), (\"Jane\", None, \"new orleans\"), (\"\", 25, \"berlin\"), (\"\", None, \"munich\")]\n",
    "schema = [\"name\", \"age\", \"office\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Initialize TransformationManager and apply transformations\n",
    "transformator = TransformationManager()\n",
    "transformations = [\n",
    "    {\n",
    "        \"name\": \"validate_fields\",\n",
    "        \"type\": \"validate_fields\",\n",
    "        \"input\": \"test_input\",\n",
    "        \"params\": {\n",
    "            \"validations\": [\n",
    "                {\"field\": \"name\", \"validations\": [\"notEmpty\"]},\n",
    "                {\"field\": \"age\", \"validations\": [\"notNull\"]}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ok_with_date\",\n",
    "        \"type\": \"add_fields\",\n",
    "        \"input\": \"test_input\",\n",
    "        \"params\": {\n",
    "          \"addFields\": [\n",
    "            {\n",
    "              \"name\": \"dt\",\n",
    "              \"function\": \"current_timestamp\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "]\n",
    "\n",
    "transformed_df = transformator.apply_transformations(df, transformations, \"test_input\")\n",
    "# Validate transformation\n",
    "assert \"validation_errors\" in transformed_df.columns, \"Validation errors column missing\"\n",
    "assert transformed_df.filter((col(\"office\") == \"new york\") & (col(\"validation_errors\") == \"[]\")).count() == 1, \"validation error should be empty in correct row\"\n",
    "assert transformed_df.filter((col(\"office\") == \"new orleans\") & (col(\"validation_errors\").contains(\"notNull\"))).count() == 1, \"validation error should contain a not null error\"\n",
    "assert transformed_df.filter((col(\"office\") == \"berlin\") & (col(\"validation_errors\").contains(\"notEmpty\"))).count() == 1, \"validation error should contain a not empty error\"\n",
    "assert transformed_df.filter((col(\"office\") == \"munich\") & (col(\"validation_errors\").contains(\"notEmpty\")) & (col(\"validation_errors\").contains(\"notNull\"))).count() == 1, \"validation error should contain a not null and not empty error\"\n",
    "assert \"dt\" in transformed_df.columns, \"Timestamp column missing\"\n",
    "\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non existing validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown validation rule: weirdVal\n",
      "Error during validation: Unknown validation rule: weirdVal\n",
      "Error applying transformation validate_fields: Error during validation: Unknown validation rule: weirdVal\n"
     ]
    }
   ],
   "source": [
    "from transformations.TransformationManager import TransformationManager\n",
    "from pyspark.sql.functions import col\n",
    "# Sample data and schema\n",
    "data = [(\"John\", 30, \"new york\"), (\"Jane\", None, \"new orleans\"), (\"\", 25, \"berlin\"), (\"\", None, \"munich\")]\n",
    "schema = [\"name\", \"age\", \"office\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Initialize TransformationManager and apply transformations\n",
    "transformator = TransformationManager()\n",
    "transformations = [\n",
    "    {\n",
    "        \"name\": \"validate_fields\",\n",
    "        \"type\": \"validate_fields\",\n",
    "        \"input\": \"test_input\",\n",
    "        \"params\": {\n",
    "            \"validations\": [\n",
    "                {\"field\": \"name\", \"validations\": [\"notEmpty\"]},\n",
    "                {\"field\": \"age\", \"validations\": [\"weirdVal\"]}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ok_with_date\",\n",
    "        \"type\": \"add_fields\",\n",
    "        \"input\": \"test_input\",\n",
    "        \"params\": {\n",
    "          \"addFields\": [\n",
    "            {\n",
    "              \"name\": \"dt\",\n",
    "              \"function\": \"current_timestamp\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "]\n",
    "try:\n",
    "    transformed_df = transformator.apply_transformations(df, transformations, \"test_input\")\n",
    "except Exception as e:\n",
    "    assert \"Unknown validation rule\" in str(e), \"unknown validation error not caught\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unkown transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown transformation rule: delete_column\n",
      "Error applying transformation delete_column: Unknown transformation rule: delete_column\n"
     ]
    }
   ],
   "source": [
    "from transformations.TransformationManager import TransformationManager\n",
    "from pyspark.sql.functions import col\n",
    "# Sample data and schema\n",
    "data = [(\"John\", 30, \"new york\"), (\"Jane\", None, \"new orleans\"), (\"\", 25, \"berlin\"), (\"\", None, \"munich\")]\n",
    "schema = [\"name\", \"age\", \"office\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Initialize TransformationManager and apply transformations\n",
    "transformator = TransformationManager()\n",
    "transformations = [\n",
    "    {\n",
    "        \"name\": \"validate_fields\",\n",
    "        \"type\": \"validate_fields\",\n",
    "        \"input\": \"test_input\",\n",
    "        \"params\": {\n",
    "            \"validations\": [\n",
    "                {\"field\": \"name\", \"validations\": [\"notEmpty\"]},\n",
    "                {\"field\": \"age\", \"validations\": [\"notNull\"]}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ok_with_date\",\n",
    "        \"type\": \"delete_column\",\n",
    "        \"input\": \"test_input\",\n",
    "        \"params\": {\n",
    "          \"delete_column\": ['age']\n",
    "        }\n",
    "      }\n",
    "]\n",
    "try:\n",
    "    transformed_df = transformator.apply_transformations(df, transformations, \"test_input\")\n",
    "except Exception as e:\n",
    "    assert \"Unknown transformation rule\" in str(e), \"unknown validation error not caught\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writer testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it writes to the three possible paths ok, ko, default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[name: string, age: int, office: string, dt: timestamp] csv ['./data/output/ok/person']\n",
      "DataFrame[name: string, age: int, office: string, validation_errors: string, dt: timestamp] json ['./data/output/ko/person']\n",
      "DataFrame[name: string, age: int, office: string] parquet ['./data/output/all/person']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "from input.Reader import Reader\n",
    "from transformations.TransformationManager import TransformationManager\n",
    "from output.Writer import Writer\n",
    "\n",
    "shutil.rmtree(\"./data/output\")\n",
    "\n",
    "# Load metadata for the workflow\n",
    "metadata_path = \"./metadata/test_writer.json\"\n",
    "with open(metadata_path, \"r\") as file:\n",
    "    metadata = json.load(file)\n",
    "\n",
    "# Initialize components\n",
    "reader = Reader(spark)\n",
    "transformator = TransformationManager()\n",
    "writer = Writer(spark)\n",
    "\n",
    "# Execute the workflow\n",
    "sources = reader.load_df(metadata[\"sources\"])\n",
    "\n",
    "# Apply transformations\n",
    "for source_name, source in sources.items():\n",
    "    sources[source_name] = transformator.apply_transformations(\n",
    "        df=source, \n",
    "        transformations=metadata[\"transformations\"], \n",
    "        input_name=source_name\n",
    "    )\n",
    "\n",
    "# Write data to sinks\n",
    "writer.write_dataframes(sources, metadata[\"sinks\"])\n",
    "\n",
    "# Validate outputs\n",
    "output_ok = \"./data/output/ok/person\"\n",
    "output_ko = \"./data/output/ko/person\"\n",
    "all = \"./data/output/all/person\"\n",
    "assert os.path.exists(output_ok), \"Output for valid data (ok) was not created\"\n",
    "assert os.path.exists(output_ko), \"Output for invalid data (ko) was not created\"\n",
    "assert os.path.exists(all), \"Output for default data (wrong path) was not created\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End to end test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletedProcess(args=['c:\\\\Users\\\\andres.langoyo\\\\Documents\\\\prueba_tecnica\\\\dataflow\\\\dt-technical-proyect\\\\.venv\\\\Scripts\\\\python.exe', './dataflow.py', '-m', './metadata/conf.json', '-l', './logs/tests_logs.txt'], returncode=0, stdout=b\"DataFrame[name: string, age: int, office: string, dt: timestamp] csv ['./data/output/ok/person']\\r\\nDataFrame[name: string, age: int, office: string, validation_errors: string, dt: timestamp] json ['./data/output/ko/person']\\r\\nCORRECTO: el proceso con PID 30984 (proceso secundario de PID 20212)\\r\\nha sido terminado.\\r\\nCORRECTO: el proceso con PID 20212 (proceso secundario de PID 5884)\\r\\nha sido terminado.\\r\\nCORRECTO: el proceso con PID 5884 (proceso secundario de PID 35808)\\r\\nha sido terminado.\\r\\n\", stderr=b'Setting default log level to \"WARN\".\\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\\r\\n24/12/20 13:54:58 WARN Utils: Service \\'SparkUI\\' could not bind on port 4040. Attempting port 4041.\\r\\n24/12/20 13:54:58 WARN Utils: Service \\'SparkUI\\' could not bind on port 4041. Attempting port 4042.\\r\\n2024-12-20 13:54:58,655 - ./metadata/conf.json - Reader - INFO - Schema conversion successful: StructType([StructField(\\'name\\', StringType(), nullable=True),StructField(\\'age\\', IntegerType(), nullable=True),StructField(\\'office\\', StringType(), nullable=True)])\\r\\n2024-12-20 13:54:58,657 - ./metadata/conf.json - Reader - INFO - Options rertrieved: {\\'dropFieldIfAllNull\\': False}\\r\\n2024-12-20 13:55:01,765 - ./metadata/conf.json - Reader - INFO - Data sources loaded successfully.\\r\\n2024-12-20 13:55:01,765 - ./metadata/conf.json - TransformationManager - INFO - Applying transformation: validate_fields with params: {\\'validations\\': [{\\'field\\': \\'office\\', \\'validations\\': [\\'notEmpty\\']}, {\\'field\\': \\'age\\', \\'validations\\': [\\'notNull\\']}]}\\r\\n2024-12-20 13:55:02,137 - ./metadata/conf.json - TransformationManager - INFO - Applying transformation: add_fields with params: {\\'addFields\\': [{\\'name\\': \\'dt\\', \\'function\\': \\'current_timestamp\\'}]}\\r\\n2024-12-20 13:55:02,137 - ./metadata/conf.json - AddFields - INFO - Adding current timestamp column \\'dt\\'.\\r\\n2024-12-20 13:55:02,155 - ./metadata/conf.json - AddFields - INFO - Added field \\'dt\\' with function \\'current_timestamp\\'.\\r\\n2024-12-20 13:55:02,155 - ./metadata/conf.json - TransformationManager - INFO - All transformations were applied successfully.\\r\\n2024-12-20 13:55:02,225 - ./metadata/conf.json - Writer - INFO - Processing sink \\'ok\\' for input \\'person_inputs\\'.\\r\\n2024-12-20 13:55:02,225 - ./metadata/conf.json - Writer - INFO - Writing DataFrame to path: ./data/output/ok/person with format: csv and save mode: OVERWRITE\\r\\n\\r[Stage 0:>                                                          (0 + 1) / 1]\\r\\r                                                                                \\r2024-12-20 13:55:05,755 - ./metadata/conf.json - Writer - INFO - Successfully wrote DataFrame to path: ./data/output/ok/person\\r\\n2024-12-20 13:55:05,755 - ./metadata/conf.json - Writer - INFO - Successfully processed sink \\'ok\\' for input \\'person_inputs\\'.\\r\\n2024-12-20 13:55:05,803 - ./metadata/conf.json - Writer - INFO - Processing sink \\'ko\\' for input \\'person_inputs\\'.\\r\\n2024-12-20 13:55:05,806 - ./metadata/conf.json - Writer - INFO - Writing DataFrame to path: ./data/output/ko/person with format: json and save mode: OVERWRITE\\r\\n2024-12-20 13:55:06,588 - ./metadata/conf.json - Writer - INFO - Successfully wrote DataFrame to path: ./data/output/ko/person\\r\\n2024-12-20 13:55:06,591 - ./metadata/conf.json - Writer - INFO - Successfully processed sink \\'ko\\' for input \\'person_inputs\\'.\\r\\n2024-12-20 13:55:06,615 - ./metadata/conf.json - py4j.clientserver - INFO - Closing down clientserver connection\\r\\n')\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "metadata_path = \"./metadata/conf.json\"\n",
    "logs_path = \"./logs/tests_logs.txt\"\n",
    "\n",
    "# Run the main script\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"./dataflow.py\", \"-m\", metadata_path, \"-l\", logs_path],\n",
    "    capture_output=True,\n",
    "    cwd=os.path.dirname(os.path.abspath('./dt-technical-proyect'))\n",
    ")\n",
    "print(result)\n",
    "# Check the logs for success\n",
    "assert result.returncode == 0, \"error occurred while running the subprocess\"\n",
    "with open(logs_path, \"r\") as log_file:\n",
    "    logs = log_file.read()\n",
    "    assert \"Successfully processed sink\" in logs, \"process didn't finish successfully\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
